{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install PyPDF2 transformers rank_bm25 tiktoken"
      ],
      "metadata": {
        "id": "Lo9c6DXzubJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a059dd10-6489-4f6e-fcd6-58e14bff1214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.51.2\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rank_bm25, PyPDF2, tiktoken\n",
            "Successfully installed PyPDF2-3.0.1 rank_bm25-0.2.2 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    print(f\"Extracting text from PDF: {pdf_path}\")\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            text += reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "pdf_path = \"dummy.pdf\"\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "print(f\"Extracted {len(text)} characters from PDF\")\n",
        "print(f\"Extracted {len(text.split())} words from PDF\")\n",
        "print('-----------------------------------PDF Text--------------------------------------')\n",
        "print(text[:200])\n",
        "print('-----------------------------------END--------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcbpSUBdbYkN",
        "outputId": "7c8c66d4-419c-4511-e218-f7fd2b5eb768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from PDF: dummy.pdf\n",
            "Extracted 13939 characters from PDF\n",
            "Extracted 1983 words from PDF\n",
            "-----------------------------------PDF Text--------------------------------------\n",
            "Certainly! Let's further expand on each section with more detailed explanations, examples, and applications across different domains. We'll aim \n",
            "to increase the level of detail to reach the desired le\n",
            "-----------------------------------END--------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_chunks(text, chunk_size=300):\n",
        "    print(f\"Splitting text into chunks of size {chunk_size}\")\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "chunks = split_into_chunks(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "996K8H5_bsqn",
        "outputId": "64d57408-e7a0-485d-d450-752b60136a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting text into chunks of size 300\n",
            "Created 7 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UNCACHED_TOKEN_PRICE = 0.15 / 1000000\n",
        "CACHED_TOKEN_PRICE = 0.08 / 1000000\n",
        "COMPLETION_TOKEN_PRICE = 0.6 / 1000000\n",
        "\n",
        "print('Price per one million tokens for prompts:')\n",
        "print(f\"Cache Token Cost: ${CACHED_TOKEN_PRICE:.8f}, \\nUncached Token Cost: ${UNCACHED_TOKEN_PRICE:.8f}, \\nCompletion Token Cost: ${COMPLETION_TOKEN_PRICE:.8f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8sue3dLb_Fv",
        "outputId": "b7ee0b25-6235-4733-ce45-ccae3d983d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price per one million tokens for prompts:\n",
            "Cache Token Cost: $0.00000008, \n",
            "Uncached Token Cost: $0.00000015, \n",
            "Completion Token Cost: $0.00000060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def count_tokens(text, model=\"gpt-4o-mini\"):\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    tokens = enc.encode(text)\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "lsSQdVXjdSJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "openai_key = userdata.get('openai_key')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "aSQJo2aUud74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcFcLEcQt4Xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846e5e22-1beb-4158-fd5b-923bba0e7ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating contextual embeddings...\n",
            "Processing chunk 1/7\n",
            "Prompt tokens: 3006, Uncached tokens: 190, Cached tokens: 2816, Completion tokens: 42\n",
            "Cost with caching: $0.000279, Cost without caching: $0.000476\n",
            "Processing chunk 2/7\n",
            "Prompt tokens: 2997, Uncached tokens: 181, Cached tokens: 2816, Completion tokens: 59\n",
            "Cost with caching: $0.000288, Cost without caching: $0.000485\n",
            "Processing chunk 3/7\n",
            "Prompt tokens: 3010, Uncached tokens: 194, Cached tokens: 2816, Completion tokens: 48\n",
            "Cost with caching: $0.000283, Cost without caching: $0.000480\n",
            "Processing chunk 4/7\n",
            "Prompt tokens: 3007, Uncached tokens: 191, Cached tokens: 2816, Completion tokens: 48\n",
            "Cost with caching: $0.000283, Cost without caching: $0.000480\n",
            "Processing chunk 5/7\n",
            "Prompt tokens: 3015, Uncached tokens: 199, Cached tokens: 2816, Completion tokens: 57\n",
            "Cost with caching: $0.000289, Cost without caching: $0.000486\n",
            "Processing chunk 6/7\n",
            "Prompt tokens: 3007, Uncached tokens: 191, Cached tokens: 2816, Completion tokens: 49\n",
            "Cost with caching: $0.000283, Cost without caching: $0.000480\n",
            "Processing chunk 7/7\n",
            "Prompt tokens: 2868, Uncached tokens: 180, Cached tokens: 2688, Completion tokens: 62\n",
            "Cost with caching: $0.000279, Cost without caching: $0.000467\n",
            "Created 7 contextual embeddings\n"
          ]
        }
      ],
      "source": [
        "total_cost_with_caching = 0\n",
        "total_cost_without_caching = 0\n",
        "\n",
        "def openai_query(system_prompt, user_prompt):\n",
        "    global total_cost_with_caching, total_cost_without_caching\n",
        "\n",
        "    user_prompt = user_prompt + 'Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else'\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": 'Document: ' + system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    prompt_tokens = completion.usage.prompt_tokens if hasattr(completion.usage, 'prompt_tokens') else 0\n",
        "    cached_tokens = completion.usage.prompt_tokens_details.cached_tokens if hasattr(completion.usage, 'prompt_tokens_details') else 0\n",
        "    uncached_tokens = prompt_tokens - cached_tokens\n",
        "    completion_tokens = count_tokens(completion.choices[0].message.content)\n",
        "\n",
        "    # Calculate cost\n",
        "    cost_with_caching = (cached_tokens * CACHED_TOKEN_PRICE) + (uncached_tokens * UNCACHED_TOKEN_PRICE) + (completion_tokens * COMPLETION_TOKEN_PRICE)\n",
        "    cost_without_caching = (prompt_tokens * UNCACHED_TOKEN_PRICE) + (completion_tokens * COMPLETION_TOKEN_PRICE)\n",
        "\n",
        "    total_cost_with_caching += cost_with_caching\n",
        "    total_cost_without_caching += cost_without_caching\n",
        "\n",
        "    print(f\"Prompt tokens: {prompt_tokens}, Uncached tokens: {uncached_tokens}, Cached tokens: {cached_tokens}, Completion tokens: {completion_tokens}\")\n",
        "    print(f\"Cost with caching: ${cost_with_caching:.6f}, Cost without caching: ${cost_without_caching:.6f}\")\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "def create_contextual_embeddings(chunks, document):\n",
        "    print(\"Creating contextual embeddings...\")\n",
        "    contextual_chunks = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
        "        response = openai_query(document, chunk)\n",
        "        contextual_chunks.append(response)\n",
        "    print(f\"Created {len(contextual_chunks)} contextual embeddings\")\n",
        "    return contextual_chunks\n",
        "\n",
        "\n",
        "contextual_chunks = create_contextual_embeddings(chunks, text)\n",
        "percentage_savings = ((total_cost_without_caching - total_cost_with_caching) / total_cost_without_caching) * 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total cost with caching: ${total_cost_with_caching:.6f}\")\n",
        "print(f\"Total cost without caching: ${total_cost_without_caching:.6f}\")\n",
        "print(f\"Savings from caching: ${total_cost_without_caching - total_cost_with_caching:.6f}\")\n",
        "print(f\"Percentage savings from caching: {percentage_savings:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i41XFqIwdBXB",
        "outputId": "2b1d20a6-8b1f-4151-a3ae-221e61cd271e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total cost with caching: $0.001985\n",
            "Total cost without caching: $0.003355\n",
            "Savings from caching: $0.001371\n",
            "Percentage savings from caching: 40.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ib7jDm_Cf4kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}